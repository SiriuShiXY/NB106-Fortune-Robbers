{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   \n",
    "from fake_useragent import UserAgent \n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入url，输出html\n",
    "def fetch_url_content(ip, port, url, headers):\n",
    "    response = requests.get(ip, port, url, headers=headers)\n",
    "    response.encoding = response.apparent_encoding\n",
    "    html = response.text\n",
    "    print(response)\n",
    "    return html\n",
    "\n",
    "# 用自己真实的UserAgent\n",
    "headers = {\"User-Agent\": 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': 200, 'data': [{'ip': '172.65.64.119', 'port': 33224}, {'ip': '172.65.64.119', 'port': 63224}, {'ip': '172.65.64.119', 'port': 43224}, {'ip': '172.65.64.119', 'port': 8240}, {'ip': '172.65.64.119', 'port': 13240}, {'ip': '172.65.64.119', 'port': 18240}, {'ip': '172.65.64.118', 'port': 56180}, {'ip': '172.65.64.119', 'port': 58240}, {'ip': '172.65.64.118', 'port': 31180}, {'ip': '172.65.64.119', 'port': 53240}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "\n",
    "# 示例 API Endpoint\n",
    "url = 'http://api.shenlongproxy.com/ip?cty=00&c=10&pt=1&ft=json&pat=\\n&rep=1&key=7d868b50&ts=3'\n",
    "\n",
    "\n",
    "try:\n",
    "    # 发送 GET 请求（可以根据 API 文档选择合适的请求方法）\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # 检查响应状态码\n",
    "    if response.status_code == 200:\n",
    "        # 解析 JSON 响应数据\n",
    "        data = response.json()\n",
    "        # 处理返回的数据\n",
    "        print(data)\n",
    "    else:\n",
    "        # 处理错误情况\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request Error: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# 将数据转化为IP池\n",
    "ip_pool = [(entry['ip'], entry['port']) for entry in data['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "crawling page 25801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   0%|          | 1/1200 [00:00<12:58,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "\n",
      "No content found!!!!!\n",
      "=====================================================\n",
      "crawling page 25802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   0%|          | 2/1200 [00:01<12:10,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "\n",
      "No content found!!!!!\n",
      "=====================================================\n",
      "crawling page 25803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   0%|          | 3/1200 [00:01<12:40,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "\n",
      "No content found!!!!!\n",
      "=====================================================\n",
      "crawling page 25804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   0%|          | 3/1200 [00:02<16:31,  1.21it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m ip \u001b[38;5;241m=\u001b[39m random_ip[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     49\u001b[0m port \u001b[38;5;241m=\u001b[39m random_ip[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 50\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_url_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m article \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[19], line 17\u001b[0m, in \u001b[0;36mfetch_url_content\u001b[1;34m(ip, port, url, headers)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders, proxies\u001b[38;5;241m=\u001b[39mproxies)\n\u001b[1;32m---> 17\u001b[0m     response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapparent_encoding\u001b[49m\n\u001b[0;32m     18\u001b[0m     html \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\requests\\models.py:793\u001b[0m, in \u001b[0;36mResponse.apparent_encoding\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapparent_encoding\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    792\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The apparent encoding, provided by the charset_normalizer or chardet libraries.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchardet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\chardet\\__init__.py:38\u001b[0m, in \u001b[0;36mdetect\u001b[1;34m(byte_str)\u001b[0m\n\u001b[0;32m     36\u001b[0m         byte_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(byte_str)\n\u001b[0;32m     37\u001b[0m detector \u001b[38;5;241m=\u001b[39m UniversalDetector()\n\u001b[1;32m---> 38\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m detector\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\chardet\\universaldetector.py:211\u001b[0m, in \u001b[0;36mUniversalDetector.feed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_charset_probers\u001b[38;5;241m.\u001b[39mappend(Latin1Prober())\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prober \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_charset_probers:\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mprober\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_str\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m ProbingState\u001b[38;5;241m.\u001b[39mFOUND_IT:\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m: prober\u001b[38;5;241m.\u001b[39mcharset_name,\n\u001b[0;32m    213\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m: prober\u001b[38;5;241m.\u001b[39mget_confidence(),\n\u001b[0;32m    214\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m: prober\u001b[38;5;241m.\u001b[39mlanguage}\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\chardet\\charsetgroupprober.py:71\u001b[0m, in \u001b[0;36mCharSetGroupProber.feed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prober\u001b[38;5;241m.\u001b[39mactive:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mprober\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\chardet\\sbcharsetprober.py:105\u001b[0m, in \u001b[0;36mSingleByteCharSetProber.feed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecedence_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seq_counters[model] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_order \u001b[38;5;241m=\u001b[39m order\n\u001b[0;32m    107\u001b[0m charset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharset_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m ProbingState\u001b[38;5;241m.\u001b[39mDETECTING:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests   \n",
    "from fake_useragent import UserAgent \n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fetch_url_content(ip, port, url, headers):\n",
    "    proxies = {\n",
    "        'http': f'http://{ip}:{port}'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, proxies=proxies)\n",
    "        response.encoding = response.apparent_encoding\n",
    "        html = response.text\n",
    "        print(response)\n",
    "        return html\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return None\n",
    "\n",
    "# 用自己真实的UserAgent\n",
    "headers = {\"User-Agent\": 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}\n",
    "\n",
    "\"\"\"自定义页码范围\"\"\"\n",
    "start_page = 25801\n",
    "end_page = 27000\n",
    "\n",
    "articles = []\n",
    "\n",
    "# Read the xlsx file\n",
    "df = pd.read_excel(r\"c:\\Users\\yfxx_\\Documents\\WeChat Files\\wxid_i2ys6cktfb5x11\\FileStorage\\File\\2024-06\\副本沪深300.xlsx\")\n",
    "\n",
    "count = 0\n",
    "threshold = 30\n",
    "\n",
    "# 使用tqdm显示进度条\n",
    "for page in tqdm(range(start_page, end_page+1), desc=\"Crawling pages\"):\n",
    "    print(\"=====================================================\")\n",
    "    print(f\"crawling page {page}\")\n",
    "    url = df.at[page-1, \"Comment_URL\"]\n",
    "\n",
    "    if url.startswith(\"https://guba.eastmoney\"):\n",
    "        random_ip = random.choice(ip_pool)\n",
    "        ip = random_ip[0]\n",
    "        port = random_ip[1]\n",
    "        html = fetch_url_content(ip, port, url, headers)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        article = []\n",
    "        contents = soup.find_all(\"div\", class_=\"newstext\")\n",
    "\n",
    "        for para in contents:\n",
    "            article.append(para.text)\n",
    "\n",
    "        result = \"\".join(article)\n",
    "        print(result)\n",
    "        df.at[page-1, \"Contents\"] = result\n",
    "\n",
    "        if not contents:\n",
    "            print(\"No content found!!!!!\")\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    if count >= threshold:\n",
    "        count = 0\n",
    "        ua = UserAgent()\n",
    "        headers = {\"User-Agent\": ua.random}\n",
    "        # sleep_time = random.randint(1500, 1800)\n",
    "        # print(f\"sleeping for {sleep_time} seconds...\")\n",
    "        # time.sleep(sleep_time)\n",
    "            \n",
    "df.to_excel(r\"c:\\Users\\yfxx_\\Documents\\guba25801-27000.xlsx\", index = False)\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9930/12401 [13:58:53<4:17:10,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "补缺完毕\n",
      "=====================================================\n",
      "crawling page 22330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9931/12401 [13:58:59<4:10:13,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "[满仓][满仓][满仓][满仓][看多][看多][抄底][抄底][满仓][满仓][满仓][看多][满仓][满仓][加仓][加仓][满仓][满仓][买入][满仓][满仓][满仓][看多][满仓][满仓][满仓][满仓][满仓][满仓][加仓][满仓][满仓][满仓][看多][看多][满仓][满仓][满仓][满仓][满仓][满仓][看多][看多][满仓][满仓][满仓][满仓][满仓][满仓][满仓][满仓][满仓]\n",
      "=====================================================\n",
      "crawling page 22331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9932/12401 [13:59:04<3:55:15,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "今天缺口全部封闭了，认为上半年的经济会如何？7连阳后可能会7连阴。\n",
      "=====================================================\n",
      "crawling page 22332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9933/12401 [13:59:07<3:26:27,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "连沪深300都补缺口了，为什么你沪指还不补缺口啊，你看看中小创都反转了啊\n",
      "=====================================================\n",
      "crawling page 22333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9934/12401 [13:59:17<4:22:25,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "银佬，工商银行，反抽犹豫不绝，有反抽空问，但上行压力不小，做足空间，保障大盘，隐，费尽心机，，\n",
      "=====================================================\n",
      "crawling page 22334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9935/12401 [13:59:22<4:12:24,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "菠罗的海运指数，反应过度了，提前维隐，哈哈，拉上，短期必回踩，\n",
      "=====================================================\n",
      "crawling page 22335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: HTTPSConnectionPool(host='guba.eastmoney.com', port=443): Max retries exceeded with url: /news,zssz399300,903372744.html (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: HTTPSConnectionPool(host='guba.eastmoney.com', port=443): Max retries exceeded with url: /news,zssz399300,903372744.html (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9936/12401 [13:59:42<6:54:15, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "波罗地海运指，反抽不隐\n",
      "=====================================================\n",
      "crawling page 22336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9937/12401 [13:59:47<5:56:39,  8.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "权重板块，不轮动，指望那些高，大，尚，的豆芽盘，逗小散，太荀了\n",
      "=====================================================\n",
      "crawling page 22337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9938/12401 [13:59:53<5:28:28,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "万科，，分时有问题\n",
      "=====================================================\n",
      "crawling page 22338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9939/12401 [14:00:10<7:12:58, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "告诫万科，冲高放量，分时直拉，图谋不轨，放量出货，顺便，拉指数，，，\n",
      "=====================================================\n",
      "crawling page 22339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9940/12401 [14:00:16<6:16:34,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "万科，分时向上，跳空直拉，分时放巨量，巨量有问题，若维特不住，出货局没跑\n",
      "=====================================================\n",
      "crawling page 22340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9941/12401 [14:00:22<5:37:40,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "万科，分阶段拉高货，，极度小心，，\n",
      "=====================================================\n",
      "crawling page 22341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9942/12401 [14:00:27<4:58:45,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "觉得这次下砸真的很精准，直接在周线60日线上开盘然后就一路上升到底是哪些人这么精准，怎么就能直接开在60日线上好像操作一样不多不少就在60线。\n",
      "=====================================================\n",
      "crawling page 22342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9943/12401 [14:00:33<4:44:50,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "这是有人拿着枪顶脑门上逼着上涨的节奏啊\n",
      "=====================================================\n",
      "crawling page 22343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9944/12401 [14:00:39<4:29:40,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "拒绝回调，前高附近才能有压力了。主力这样涨吓死胆小的\n",
      "=====================================================\n",
      "crawling page 22344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9945/12401 [14:00:43<3:58:23,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "【0113沪深300期权操盘日志】1.预判:看不清楚了；2.今天没有跌破3937.22，指数继续向上逼空，这个位置看多也不敢做多；3.策略:还在降波，认沽不急，逢高分批干，即使被套也睡得着。\n",
      "=====================================================\n",
      "crawling page 22345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9946/12401 [14:00:49<4:02:21,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "八连阳了，兄弟，不考虑歇歇？\n",
      "=====================================================\n",
      "crawling page 22346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9947/12401 [14:00:52<3:27:59,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "今日300股指所有期权成交量为36470,持仓量为78343\n",
      "=====================================================\n",
      "crawling page 22347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9948/12401 [14:00:56<3:14:02,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "可能没几个敢，出了价值投资者，是用估值来买卖的，我开年开盘第一天快收盘时大量买进，只有羊群那天才会卖出的。那天可是北上资金买入100多亿，当然长线价值投资并不会在乎这几天的收益\n",
      "=====================================================\n",
      "crawling page 22348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9949/12401 [14:01:07<4:22:55,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "预警，趋势线跌破。\n",
      "=====================================================\n",
      "crawling page 22349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Crawling pages:  80%|████████  | 9950/12401 [14:01:10<3:48:52,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "老唐复盘：疫情，真不知道韭菜在想什么\n",
      "=====================================================\n",
      "crawling page 22350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.shenlongproxy.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests   \n",
    "from fake_useragent import UserAgent \n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fetch_url_content(url, headers):\n",
    "    targetUrl = \"http://myip.ipip.net\"\n",
    "\n",
    "    proxyHost = \"proxy.shenlongproxy.com\"\n",
    "    proxyPort = 31212\n",
    "\n",
    "    #非账号密码验证\n",
    "    # proxyMeta = \"http://%(host)s:%(port)s\" % {\n",
    "\n",
    "    #     \"host\": proxyHost,\n",
    "    #     \"port\": proxyPort,\n",
    "    # }\n",
    "    # 账号密码验证\n",
    "    account = \"customer-5b90a45c994-country-HK\"\n",
    "    password = \"ff02b54c\"\n",
    "    proxyMeta = f\"http://{account}:{password}@{proxyHost}:{proxyPort}\"\n",
    "\n",
    "\n",
    "    proxies = {\n",
    "        \"http\": proxyMeta,\n",
    "        \"https\": proxyMeta\n",
    "    }\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, proxies=proxies, verify = False)\n",
    "            response.encoding = response.apparent_encoding\n",
    "            html = response.text\n",
    "            print(response)\n",
    "            return html\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching URL: {e}\")\n",
    "\n",
    "\n",
    "headers = {\"User-Agent\": 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}\n",
    "\n",
    "\"\"\"自定义页码范围\"\"\"\n",
    "start_page = 12400\n",
    "end_page = 24800\n",
    "\n",
    "articles = []\n",
    "\n",
    "# Read the xlsx file\n",
    "df = pd.read_excel(r\"c:\\Users\\yfxx_\\Documents\\WeChat Files\\wxid_i2ys6cktfb5x11\\FileStorage\\File\\2024-06\\副本沪深300.xlsx\")\n",
    "\n",
    "count = 0\n",
    "threshold = 30\n",
    "\n",
    "# 使用tqdm显示进度条\n",
    "for page in tqdm(range(start_page, end_page+1), desc=\"Crawling pages\"):\n",
    "    print(\"=====================================================\")\n",
    "    print(f\"crawling page {page}\")\n",
    "    url = df.at[page-1, \"Comment_URL\"]\n",
    "\n",
    "    if url.startswith(\"https://guba.eastmoney\"):\n",
    "        while True:\n",
    "            try:\n",
    "                html = fetch_url_content(url, headers)\n",
    "                break\n",
    "            except:\n",
    "                print(\"error\")\n",
    "                continue\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        article = []\n",
    "        contents = soup.find_all(\"div\", class_=\"newstext\")\n",
    "\n",
    "        for para in contents:\n",
    "            article.append(para.text)\n",
    "\n",
    "        result = \"\".join(article)\n",
    "        print(result)\n",
    "        df.at[page-1, \"Contents\"] = result\n",
    "\n",
    "        if not contents:\n",
    "            print(\"No content found!!!!!\")\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    if count >= threshold:\n",
    "        count = 0\n",
    "        ua = UserAgent()\n",
    "        headers = {\"User-Agent\": ua.random}\n",
    "        # sleep_time = random.randint(1500, 1800)\n",
    "        # print(f\"sleeping for {sleep_time} seconds...\")\n",
    "        # time.sleep(sleep_time)\n",
    "\n",
    "df.to_excel(r\"C:\\Users\\yfxx_\\Desktop\\NB106-Fortune-Robbers-main\\RawData\\Comments\\股吧\\12400-24800guba.xlsx\", index = False)\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Crawling page 25801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   0%|          | 1/1200 [00:00<11:30,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   0%|          | 2/1200 [00:01<12:01,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   0%|          | 3/1200 [00:01<11:23,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   0%|          | 4/1200 [00:02<11:36,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25805\n",
      "=====================================================\n",
      "Crawling page 25806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   0%|          | 6/1200 [00:02<08:42,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   1%|          | 7/1200 [00:03<09:19,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   1%|          | 8/1200 [00:04<10:18,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25809\n",
      "=====================================================\n",
      "Crawling page 25810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   1%|          | 10/1200 [00:04<08:13,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   1%|          | 11/1200 [00:05<08:49,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   1%|          | 12/1200 [00:05<09:17,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   1%|          | 13/1200 [00:06<09:41,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   1%|          | 14/1200 [00:06<09:57,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   1%|▏         | 15/1200 [00:07<10:26,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   1%|▏         | 16/1200 [00:08<11:25,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   1%|▏         | 17/1200 [00:08<11:26,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   2%|▏         | 18/1200 [00:09<11:44,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25819\n",
      "=====================================================\n",
      "Crawling page 25820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   2%|▏         | 20/1200 [00:10<09:11,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   2%|▏         | 21/1200 [00:10<09:53,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   2%|▏         | 22/1200 [00:11<11:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   2%|▏         | 23/1200 [00:12<12:13,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No content found!\n",
      "=====================================================\n",
      "Crawling page 25824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   2%|▏         | 23/1200 [00:12<10:52,  1.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m url \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mat[page \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComment_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m url\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://guba.eastmoney\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 53\u001b[0m     html \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_url_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m html:\n\u001b[0;32m     55\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m, in \u001b[0;36mfetch_url_content\u001b[1;34m(url, headers, proxies)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_url_content\u001b[39m(url, headers, proxies):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m         response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mapparent_encoding\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connectionpool.py:1096\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1096\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[0;32m   1099\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1100\u001b[0m         (\n\u001b[0;32m   1101\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconn\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1106\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1107\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connection.py:642\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_time_off:\n\u001b[0;32m    634\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    635\u001b[0m         (\n\u001b[0;32m    636\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSystem time is way off (before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRECENT_DATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). This will probably \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    639\u001b[0m         SystemTimeWarning,\n\u001b[0;32m    640\u001b[0m     )\n\u001b[1;32m--> 642\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39mis_verified\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\connection.py:782\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[0;32m    780\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 782\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[1;32mc:\\Users\\yfxx_\\.conda\\envs\\MachineLearning\\lib\\site-packages\\urllib3\\util\\ssl_.py:445\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ca_certs \u001b[38;5;129;01mor\u001b[39;00m ca_cert_dir \u001b[38;5;129;01mor\u001b[39;00m ca_cert_data:\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 445\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fetch_url_content(url, headers, proxies):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, proxies=proxies, timeout=10)\n",
    "        response.encoding = response.apparent_encoding\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Failed to fetch URL {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return None\n",
    "\n",
    "# 自定义页码范围\n",
    "start_page = 25801\n",
    "end_page = 27000\n",
    "\n",
    "# 读取xlsx文件\n",
    "df = pd.read_excel(r\"c:\\Users\\yfxx_\\Documents\\WeChat Files\\wxid_i2ys6cktfb5x11\\FileStorage\\File\\2024-06\\副本沪深300.xlsx\")\n",
    "\n",
    "count = 0\n",
    "threshold = 30\n",
    "\n",
    "# 设置初始的用户代理头部信息\n",
    "ua = UserAgent()\n",
    "headers = {\"User-Agent\": ua.random}\n",
    "\n",
    "# 设置代理信息\n",
    "proxyHost = \"proxy.shenlongproxy.com\"\n",
    "proxyPort = 31212\n",
    "account = \"customer-5c930e70408-country-HK\"\n",
    "password = \"a205de38\"\n",
    "proxyMeta = f\"http://{account}:{password}@{proxyHost}:{proxyPort}\"\n",
    "proxies = {\n",
    "    \"http\": proxyMeta,\n",
    "    # \"https\": proxyMeta  # 如果需要访问HTTPS网站，取消注释这行\n",
    "}\n",
    "\n",
    "# 使用tqdm显示进度条\n",
    "for page in tqdm(range(start_page, end_page + 1), desc=\"Crawling pages\"):\n",
    "    print(\"=====================================================\")\n",
    "    print(f\"Crawling page {page}\")\n",
    "    url = df.at[page - 1, \"Comment_URL\"]\n",
    "\n",
    "    if url.startswith(\"https://guba.eastmoney\"):\n",
    "        html = fetch_url_content(url, headers, proxies)\n",
    "        if html:\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            article = []\n",
    "            contents = soup.find_all(\"div\", class_=\"newstext\")\n",
    "\n",
    "            for para in contents:\n",
    "                article.append(para.text)\n",
    "\n",
    "            result = \"\".join(article)\n",
    "            print(result)\n",
    "            df.at[page - 1, \"Contents\"] = result\n",
    "\n",
    "            if not contents:\n",
    "                print(\"No content found!\")\n",
    "            count += 1\n",
    "\n",
    "    if count >= threshold:\n",
    "        count = 0\n",
    "        time.sleep(random.randint(1500, 1800))  # 随机休眠一段时间，模拟人类操作\n",
    "        ua = UserAgent()\n",
    "        headers = {\"User-Agent\": ua.random}\n",
    "\n",
    "# 将结果保存到Excel文件\n",
    "df.to_excel(r\"c:\\Users\\yfxx_\\Documents\\guba25801-27000.xlsx\", index=False)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
